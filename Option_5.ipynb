{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of  Option_5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jadid86/GPD-Register/blob/main/Option_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y0dmfwVfPKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d542b74a-43a1-4038-f9c0-6650461cb93d"
      },
      "source": [
        "import pandas as pd \n",
        "pd.set_option('max_rows', 99999)\n",
        "pd.set_option('max_colwidth', 400)\n",
        "pd.describe_option('max_colwidth')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "display.max_colwidth : int or None\n",
            "    The maximum width in characters of a column in the repr of\n",
            "    a pandas data structure. When the column overflows, a \"...\"\n",
            "    placeholder is embedded in the output. A 'None' value means unlimited.\n",
            "    [default: 50] [currently: 400]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zzJdyYVPZzr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ce9cc51-db13-4e5d-e119-d0ab169c2a84"
      },
      "source": [
        "import nltk \n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import urllib.request\n",
        "import requests\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#from google.colab import files\n",
        "#path=\"https://www.github.com/jadid86/GPD-Register/db(newcategory).xlsx\"\n",
        "\n",
        "!git clone https://github.com/jadid86/GPD-Register.git\n",
        "\n",
        "from google.colab import files\n",
        "#path=\"https://github.com/jadid86/GPD-Register/raw/db(newcategory).xlsx\"\n",
        "path=\"/content/GPD-Register/db(newcategory).xlsx\"\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tabulate import tabulate\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "df_action= pd.read_excel(path,sheet_name='Action Items')\n",
        "df_issue= pd.read_excel(path,sheet_name='Issues')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Cloning into 'GPD-Register'...\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 4 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (4/4), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twnzua_TUqBH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb4937ab-f7aa-4fb4-a3c9-a7f94109489d"
      },
      "source": [
        "stop = stopwords.words('english')\n",
        "lookup_dict = {'eg':'example','etc':'et cetera','hse':'health, safety and environment,','csr':'corporate social responsibility', 'wbu' : 'what about you'}\n",
        "\n",
        "def text_std(input_text):\n",
        "  words = input_text.split()\n",
        "  for word in words:\n",
        "    if word.lower() in lookup_dict:\n",
        "      word = lookup_dict[word.lower()]\n",
        "  return word\n",
        "\n",
        "def vectorize():\n",
        "  frames =[df_issue['lemmatizing'],df_action['lemmatizing']]\n",
        "  new_df =pd.concat(frames)\n",
        "  vectorizer= TfidfVectorizer()\n",
        "  new_vectorize=vectorizer.fit_transform(new_df)\n",
        "  new_issue_vec= new_vectorize[0:df_issue['lemmatizing'].shape[0]]\n",
        "  new_action_vec= new_vectorize[df_issue['lemmatizing'].shape[0]:new_vectorize.shape[0]]\n",
        "  return[new_issue_vec,new_action_vec]\n",
        "\n",
        "df_issue['Project Type']= df_issue['Project Type'].map(lambda x: x.upper())\n",
        "df_issue['Facility Type']= df_issue['Facility Type'].map(lambda x: x.upper())\n",
        "df_issue['Category']= df_issue['Category'].map(lambda x: x.upper())\n",
        "df_issue['ITBC']= df_issue['ITBC'].map(lambda x: x.upper())\n",
        "\n",
        "df_action['Project Type']= df_action['Project Type'].map(lambda x: x.upper())\n",
        "df_action['Facility Type']= df_action['Facility Type'].map(lambda x: x.upper())\n",
        "df_action['Category']= df_action['Category'].map(lambda x: x.upper())\n",
        "df_action['ITBC']= df_action['ITBC'].map(lambda x: x.upper())\n",
        "\n",
        "pname = df_issue['Project Type'].drop_duplicates()\n",
        "\n",
        "#filtering the database\n",
        "print(\"List of existing Project Type\\n\")\n",
        "print(\"\\n\",pname.reset_index(drop=True))\n",
        "\n",
        "projt_input = (input(\"\\nEnter Project Type\\n\")).upper()\n",
        "projt_df= df_issue.loc[df_issue['Project Type'] == projt_input]\n",
        "projt_df1= df_action.loc[df_action['Project Type']==projt_input]\n",
        "\n",
        "facit =(projt_df['Facility Type'].drop_duplicates())\n",
        "print(\"\\n\",facit.reset_index(drop=True))\n",
        "\n",
        "facit_input = (input(\"\\nEnter Facility Type \\n\")).upper()\n",
        "facit_df= projt_df.loc[projt_df['Facility Type'] == facit_input]\n",
        "facit_df1= projt_df1.loc[projt_df1['Facility Type'] == facit_input]\n",
        "\n",
        "more_input= (input(\"Do you want to add more facility type?\")).upper()\n",
        "more= more_input\n",
        "while more==\"YES\":\n",
        "  facit_input = (input(\"\\nEnter Facility Type \\n\")).upper()\n",
        "  ndf=projt_df.loc[projt_df['Facility Type'] == facit_input]\n",
        "  ndf1=projt_df1.loc[projt_df1['Facility Type'] == facit_input]\n",
        "  facit_df=facit_df.append(ndf)\n",
        "  facit_df1=facit_df1.append(ndf1)\n",
        "  more_input= (input(\"Do you want to add more facility type?\")).upper()\n",
        "  more=more_input\n",
        "\n",
        "category_list=facit_df['Category'].drop_duplicates()\n",
        "print(\"\\n\",category_list.reset_index(drop=True))\n",
        "\n",
        "catt_input =(input(\"\\nEnter Category \\n\")).upper()\n",
        "catt_df =facit_df.loc[facit_df['Category']==catt_input]\n",
        "catt_df1 =facit_df1.loc[facit_df1['Category']==catt_input]\n",
        "\n",
        "more_input= (input(\"Do you want to add more Category?\")).upper()\n",
        "more= more_input\n",
        "while more==\"YES\":\n",
        "  catt_input = (input(\"\\nEnter Category \\n\")).upper()\n",
        "  ndf=facit_df.loc[facit_df['Category'] == catt_input]\n",
        "  ndf1=facit_df1.loc[facit_df1['Category'] == catt_input]\n",
        "  catt_df=catt_df.append(ndf)\n",
        "  catt_df1=catt_df1.append(ndf1)\n",
        "  more_input= (input(\"Do you want to add more Category ?\")).upper()\n",
        "  more=more_input\n",
        "\n",
        "def vectorize():\n",
        "  frames =[ndf_issue['lemmatizing'],ndf_action['lemmatizing']]\n",
        "  new_df =pd.concat(frames)\n",
        "  vectorizer= TfidfVectorizer()\n",
        "  new_vectorize=vectorizer.fit_transform(new_df)\n",
        "  new_issue_vec= new_vectorize[0:ndf_issue['lemmatizing'].shape[0]]\n",
        "  new_action_vec= new_vectorize[ndf_issue['lemmatizing'].shape[0]:new_vectorize.shape[0]]\n",
        "  return[new_issue_vec,new_action_vec]\n",
        "\n",
        "\n",
        "u_input =int(input(\"Select 1 to add new ITBC or 2 to select existing ITBC:\\n\"))\n",
        "\n",
        "if u_input==1:\n",
        "  itbc_input=(input(\"\\nPlease type new ITBC: \\n\")).upper()\n",
        "  x=facit_df.loc[facit_df['ITBC']==itbc_input]\n",
        "\n",
        "  if x.empty:\n",
        "    ndf_issue=facit_df.copy()\n",
        "    ndf_action=facit_df1.copy()\n",
        "    ndf_issue = ndf_issue.reset_index(drop=True)\n",
        "    ndf_action = ndf_action.reset_index(drop=True)\n",
        "  else:\n",
        "     itbcl_df =facit_df.loc[facit_df['ITBC']==itbc_input]\n",
        "     itbcl_df1 =facit_df1.loc[facit_df1['ITBC']==itbc_input]\n",
        "     ndf_issue=itbcl_df.copy()\n",
        "     ndf_action=itbcl_df1.copy()\n",
        "\n",
        "  ndf_issue = ndf_issue.reset_index(drop=True)\n",
        "  ndf_action = ndf_action.reset_index(drop=True)\n",
        "\n",
        "  #text processing issue\n",
        "  ndf_issue['lower']= ndf_issue['Issues'].map(lambda x: x.lower())\n",
        "  ndf_issue['punct']= ndf_issue['lower'].str.replace('[^\\w\\s]','')\n",
        "  ndf_issue['stop']= ndf_issue['punct'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "  ndf_issue['standard']=ndf_issue['stop'][:500].apply(lambda x: \" \".join([text_std(word) for word in x.split()]))\n",
        "  ndf_issue['correct']=ndf_issue['standard'].apply(lambda x: str(TextBlob(x).correct()))\n",
        "  ndf_issue['lemmatizing'] =ndf_issue['correct'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "\n",
        "  #text processing action\n",
        "  ndf_action['lower']= ndf_action['Action Items'].map(lambda x: x.lower())\n",
        "  ndf_action['punct']=ndf_action['lower'].str.replace('[^\\w\\s]','')\n",
        "  ndf_action['stop']= ndf_action['punct'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "  ndf_action['standard']=ndf_action['stop'][:500].apply(lambda x: \" \".join([text_std(word) for word in x.split()]))\n",
        "  ndf_action['correct']=ndf_action['standard'].apply(lambda x: str(TextBlob(x).correct()))\n",
        "  ndf_action['lemmatizing'] = ndf_action['correct'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "\n",
        "  _action_vec=\"\"\n",
        "  issue_option=\"\"\n",
        "  text=input(\"Enter new issue :\\n \")\n",
        "  ndf_issue= ndf_issue.append({'ISSUES':text},ignore_index=True ) \n",
        "  ndf_issue['lower']= ndf_issue.apply(lambda x: x.astype(str).str.lower())\n",
        "  ndf_issue['punct']=ndf_issue['lower'].str.replace('[^\\w\\s]','')\n",
        "  ndf_issue['stop']= ndf_issue['punct'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "  ndf_issue['standard']=ndf_issue['stop'][:100].apply(lambda x: \" \".join([text_std(word) for word in x.split()]))\n",
        "  ndf_issue['correct']=ndf_issue['standard'].apply(lambda x: str(TextBlob(x).correct()))\n",
        "  ndf_issue['lemmatizing'] = ndf_issue['correct'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "  new_issue_vec,new_action_vec=vectorize()\n",
        "  issue_option = new_issue_vec[-1]\n",
        "  print(\"\\nSelected Issue : \",text)\n",
        "\n",
        "elif u_input==2:\n",
        "\n",
        "  itbc_l=facit_df['ITBC'].drop_duplicates()\n",
        "  print(\"\\n\",itbc_l.reset_index(drop=True))\n",
        "\n",
        "  itbcl_input =(input(\"\\nEnter Item to be Considered \\n\")).upper()\n",
        "  itbcl_df =facit_df.loc[facit_df['ITBC']==itbcl_input]\n",
        "  itbcl_df1 =facit_df1.loc[facit_df1['ITBC']==itbcl_input]\n",
        "\n",
        "  more_input= (input(\"Do you want to add more Item to be Considered?\")).upper()\n",
        "  more= more_input\n",
        "  while more==\"YES\":\n",
        "    itbcl_input =(input(\"\\nEnter Item to be Considered \\n\")).upper()\n",
        "    ndf=facit_df.loc[facit_df['ITBC']==itbcl_input]\n",
        "    ndf1=facit_df1.loc[facit_df1['ITBC']==itbcl_input]\n",
        "    itbcl_df=itbcl_df.append(ndf)\n",
        "    itbcl_df1=itbcl_df1.append(ndf1)\n",
        "    more_input= (input(\"Do you want to add more Item to be Considered ?\")).upper()\n",
        "    more=more_input\n",
        "\n",
        "  ndf_issue=itbcl_df.copy()\n",
        "  ndf_action=itbcl_df1.copy()\n",
        "  ndf_issue = ndf_issue.reset_index(drop=True)\n",
        "  ndf_action = ndf_action.reset_index(drop=True)\n",
        "\n",
        "  #text processing issue\n",
        "  ndf_issue['lower']= ndf_issue['Issues'].map(lambda x: x.lower())\n",
        "  ndf_issue['punct']= ndf_issue['lower'].str.replace('[^\\w\\s]','')\n",
        "  ndf_issue['stop']= ndf_issue['punct'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "  ndf_issue['standard']=ndf_issue['stop'][:500].apply(lambda x: \" \".join([text_std(word) for word in x.split()]))\n",
        "  ndf_issue['correct']=ndf_issue['standard'].apply(lambda x: str(TextBlob(x).correct()))\n",
        "  ndf_issue['lemmatizing'] =ndf_issue['correct'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "\n",
        "  #text processing action\n",
        "  ndf_action['lower']= ndf_action['Action Items'].map(lambda x: x.lower())\n",
        "  ndf_action['punct']=ndf_action['lower'].str.replace('[^\\w\\s]','')\n",
        "  ndf_action['stop']= ndf_action['punct'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "  ndf_action['standard']=ndf_action['stop'][:500].apply(lambda x: \" \".join([text_std(word) for word in x.split()]))\n",
        "  ndf_action['correct']=ndf_action['standard'].apply(lambda x: str(TextBlob(x).correct()))\n",
        "  ndf_action['lemmatizing'] = ndf_action['correct'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "\n",
        "  _action_vec=\"\"\n",
        "  issue_option=\"\"\n",
        "  display(ndf_issue['Issues'])\n",
        "  text2=int(input(\"Select issue number :\"))\n",
        "  print(\"\\n\\nSelected Issue :\",ndf_issue['Issues'][text2])\n",
        "  new_issue_vec,new_action_vec=vectorize()\n",
        "  issue_option = new_issue_vec[text2]\n",
        "\n",
        "c_similarity_Action = cosine_similarity(new_action_vec,issue_option)\n",
        "b= c_similarity_Action.tolist()\n",
        "b.sort(reverse=True)\n",
        "ndf_action['Cosine']= c_similarity_Action\n",
        "ndf_result = ndf_action[['Cosine','Action Items']].drop_duplicates()\n",
        "ndf_result=ndf_result.reset_index(drop=True)\n",
        "sort= ndf_result.sort_values(by='Cosine', ascending=False)\n",
        "display(sort)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List of existing Project Type\n",
            "\n",
            "\n",
            " 0      BROWNFIELD\n",
            "1             FIP\n",
            "2    REJUVENATION\n",
            "3      GREENFIELD\n",
            "4    PLANT CHANGE\n",
            "Name: Project Type, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}